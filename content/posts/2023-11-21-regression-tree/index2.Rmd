---
title: "Tree-based classification"
author: "Oskar Mikko"
date: "2023-11-21"
output:
  html_document:
    df_print: paged
categories: Machine Learning
tags:
- R
- Classification
- CART
- Regression
slug: []
---

```{r setup, include=FALSE,}
knitr::opts_chunk$set(collapse = TRUE,warning=FALSE,message=F)
```

# Decision trees

- The goal is to classify an outcome based on a set of predictors.

```{r}
library(tidyverse)
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(pROC)
library(tree)
library(caret)
```

## Classification Tree: Detecting email spam data


```{r}
data <- spam7
str(data)  
table(data$yesno)
```

The data consists of 4601 emails, 1813 of them were considered as spam emails.

The variables in the dataset are:
 - crl.tot: total length of words in capitals
 - dollar: number of occurrences of the $ symbol.
 - bang: number of occurrences of the ! symbol.
 - money: number of occurrences of the word 'money'
 - n000: number of occurrences of the string '000'
 - make: number of occurrences of the word 'make'
 - yesno: outcome variable; y = if spam, no = if not spam

- The goal is to classify each email as spam or not spam using the decision tree.

# Splitting data

We need to split data into training and testing for our model. 

```{r}
set.seed(1337)
index <- sample(2, nrow(data), replace = T, prob = c(0.5, 0.5))
train <- data[index == 1,]
test <- data[index == 2,]
``` 

# The tree  
```{r}
tree <- rpart(yesno ~., data = train)
rpart.plot(tree,type=4,extra=101)

```


The root node is dollar and if dollar is > 0.046 then the email is classified as spam. 27% of the data has dollar > 0.046 and 88% of that data is spam.

The second node is bang. 73% percent of the data goes through this splitting criteria that is is bang < 0.18. If bang > 0.18 and crl.tot > 78 the email is classified as spam. If bang > 0.18 and crl.tot < 78 then the data goes through an another node: bang < 1.1. If bang > 1.1 then the classification is spam. If bang < 1.1 the classification is not spam.

The remaining data that has bang < 0.18 goes through the last node: money < 0.01. If money is > 0.01 the classification becomes spam, and if money is less than 0.01 the classification becomes no spam.


# Confusion Matrix for Training Data

```{r}
p <- predict(tree, train, type = 'class')
confusionMatrix(p, train$yesno)

```
The accuracy of the model is 86.81%. 

# Confusion Matrix for Testing Data

```{r}
pp <- predict(tree, test, type = 'class')
confusionMatrix(pp, test$yesno)
```
The accuracy for the testing data is 84.92%, which is a little lower than for the training data. 

But overall the performance looks similar on both training and testing data which suggests that there are no signs of overfitting. 








