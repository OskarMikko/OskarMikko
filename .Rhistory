install.packages("dbplyr")
library(dbplyr)
library(tidyverse)
UID = rstudioapi::askForPassword("Database user")
library(RPostgreSQL)
install.packages("RPostgreSQL")
odbc::odbcListDrivers()
install.packages("odbc")
odbc::odbcListDrivers()
UID = rstudioapi::askForPassword("Database user")
rental <- DBI::dbConnect(odbc::odbc(),
driver = "PostgreSQL ANSI(x64)",
database = "dvdrental_2",
port = 5433,
host = "localhost",
UID = rstudioapi::askForPassword("Database user"))
rental_2 = DBI::dbConnect(RPostgreSQL::PostgreSQL(), "dvdrental")
library(DBI)
con <- dbConnect(odbc::odbc(), .connection_string = "Driver={SQL Server};", timeout = 10)
rental_2 = DBI::dbConnect(RPostgreSQL::PostgreSQL(), "dvdrental")
?dbConnect
rental_2 = DBI::dbConnect(RPostgreSQL::PostgreSQL(), "dvdrental",password= "84702700")
rental_2 = DBI::dbConnect(RPostgreSQL::PostgreSQL(), "dvdrental")
con <- dbConnect(RMySQL::MySQL(),
dbname = "mydb",
host = "https://predictivehacks.com/",
port = 3306,
user = rstudioapi::askForPassword("Database user"),
password = rstudioapi::askForPassword("Database password")
)
install.packages("RMySQL")
library("RMySQL")
con <- dbConnect(RMySQL::MySQL(),
dbname = "mydb",
host = "https://predictivehacks.com/",
port = 3306,
user = rstudioapi::askForPassword("Database user"),
password = rstudioapi::askForPassword("Database password")
)
con <- DBI::dbConnect(
RSQLite::SQLite(),
dbname = "020_write_sql_from_r/mpg.sqlite"
)
con <- DBI::dbConnect(
RSQLite::SQLite(),
dbname = "020_write_sql_from_r/mpg.sqlite"
)
install.packages("RSQLite")
library("RSQLite")
con <- DBI::dbConnect(
RSQLite::SQLite(),
dbname = "020_write_sql_from_r/mpg.sqlite"
)
con <- DBI::dbConnect(
RSQLite::SQLite(),
dbname = "020_write_sql_from_r/mpg.sqlite"
)
table_names <- dbListTables(con)
con <- dbConnect(odbc::odbc(), "Oracle DB")
con <- dbConnect(RSQLite::SQLite(), "mydatabase.db")
sql_file <- "GFG.sql"
sql_text <- readLines(sql_file, warn = FALSE, encoding = "UTF-8")
sql_text <- readLines(sql_file, warn = FALSE, encoding = "UTF-8")
sql_file <- "parameterized_query.sql"
sql_text <- readLines(sql_file, warn = FALSE, encoding = "UTF-8")
start_date <- "1982-01-01"
end_date <- "1983-12-31"
# Execute the SQL query with parameters and store the results in a data frame
query_result <- dbGetQuery(con, sqlInterpolate(con, paste(sql_text,
collapse = "\n"), .dots = list(start_date,
end_date)))
sql_text <- readLines(sql_file, warn = FALSE, encoding = "UTF-8")
con <- DBI::dbConnect(drv = odbc::odbc(),
Driver = "driver_name",
Server = "server_url",
Database = "database_name",
user = "user", #optional
password = "password") #optional
conn <- dbConnect(RSQLite::SQLite(), "data/mental_health.sqlite")
dbListTables(conn)
dbGetQuery(conn, "SELECT * FROM Survey")
conn <- dbConnect(SQLite(),'mycars.db')
dbWriteTable(conn, "cars", mtcars)
dbGetQuery(conn, 'CREATE TABLE test_table(id int, name text)')
dbExecute(conn, 'CREATE TABLE test_table(id int, name text)')
dbExecute(conn, 'CREATE TABLE test_t(id int, name text)')
chinook <- dbConnect(SQLite(), "chinook.db")
src_dbi(chinook)
employees <- tbl(chinook, "employees")
employees
employees <- tbl(chinook, "employees")
employees
src_dbi(chinook)
employees <- tbl(chinook, "employees")
dbDisconnect()
employees <- tbl(chinook, "employees")
install.packages("bigrquery")
library("bigrquery")
billing <- "for875-databases"
con <- dbConnect(
bigquery(),
project = "publicdata",
dataset = "samples",
billing = billing
)
shakespeare <- con %>%
tbl("shakespeare")
shakespeare %>%
group_by(word) %>%
summarise(n = sum(word_count, na.rm = TRUE)) %>%
arrange(desc(n)) %>%
head(10)
shakespeare <- con %>%
tbl("shakespeare")
con <- dbConnect(
bigquery(),
project = "publicdata",
dataset = "samples",
billing = billing
)
shakespeare <- con %>%
tbl("shakespeare")
library(httr)
library(jsonlite)
library(readr)
library(kaggler)
install.packages("kaggler")
library(kaggler)
setwd("C:/Users/Oskar/Documents/R/kaggle")
kgl_auth(creds_file = 'kaggle.json')
library(kaggler)
install.packages("kaggler")
library(kaggler)
install.packages('RcmdrPlugin.IPSUR', lib = "/kaggle/working")
library(RcmdrPlugin.IPSUR, lib = "/kaggle/working")
kgl_auth(creds_file = 'kaggle.json')
library(kaggler)
install.packages("kaggler")
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE,warning=FALSE,message=F)
data <- read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/data/london_bike_sharing_data/london_merged.csv")
head(data)
head(data)
library(tidyverse)
head(data)
ggplot(data,
aes(timestamp,cnt, color=weather_code)) + geom_line()
ggplot(data,
aes(timestamp,cnt, color=weather_code)) + geom_point()
ggplot(data,
aes(timestamp,cnt, color=as.factor(weather_code))) + geom_point()
data %>%
group_by(weather_code) %>%
summarise(cnt)
data %>%
group_by(as.factor(weather_code)) %>%
summarise(cnt)
head(data)
data$timestamp <- as.Date(data$timestamp)
str(data)
data$timestamp
?as.Date
data$date <- as.Date(data$timestamp)
data %>%
group_by(date,as.factor(weather_code)) %>%
summarise(cnt)
ggplot(data,
aes(x=date,y=cnt,group=as.factor(weather_codes)))
ggplot(data,
aes(x=date,y=cnt,group=as.factor(weather_code)))
ggplot(data,
aes(x=date,y=cnt,group=as.factor(weather_code))) + geom_line()
knitr::opts_chunk$set(collapse = TRUE,warning=FALSE,message=F)
library(tidyverse)
library(tidyverse)
data <- read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/data/london_bike_sharing_data/london_merged.csv")
head(data)
data$date <- as.Date(data$timestamp)
str(data)
data %>%
group_by(date,as.factor(weather_code)) %>%
summarise(cnt)
blogdown:::preview_site()
cnt_by_weather <- data %>%
group_by(date,as.factor(weather_code)) %>%
summarise(cnt)
cnt_by_weather
cnt_by_weather <- data %>%
group_by(date,weather_code = as.factor(weather_code)) %>%
summarise(cnt)
data %>%
mutate(date = as.Date(data$timestamp),
weather_code = as.factor(data$weather_code))
data <- data %>%
mutate(date = as.Date(data$timestamp),
weather_code = as.factor(data$weather_code))
cnt_by_weather <- data %>%
group_by(date,weather_code) %>%
summarise(cnt)
ggplot(cnt_by_weather,
aes(x=date,y=cnt,group=weather_code)) + geom_line()
ggplot(cnt_by_weather,
aes(x=date,y=cnt,group=weather_code,color=weater_code)) + geom_line()
ggplot(cnt_by_weather,
aes(x=date,y=cnt,group=weather_code,color=weather_code)) + geom_line()
cnt_by_weather <- data %>%
group_by(date,weather_code) %>%
summarise(cnt)
cnt_by_weather
lm(cnt~weather_code,data=cnt_by_weather)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(cnt)
cnt_by_weather
cnt_by_weather
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(cnt)
cnt_by_weather
data %>%
group_by(weather_code) %>%
summarise(cnt)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(sum(cnt))
cnt_by_weather
head(data)
lm(cnt~*,data=cnt_by_weather)
?lm
lm(cnt~ ,data=cnt_by_weather -1)
lm(cnt~ ,data=cnt_by_weather)
lm(cnt~weather_code -1,data=cnt_by_weather)
cnt_by_weather
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(cnt)
cnt_by_weather
lm(cnt~weather_code -1,data=cnt_by_weather)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt))
cnt_by_weather
blogdown::hugo_build()
blogdown::serve_site()
blogdown::build_site()
blogdown::build_site()
blogdown::hugo_build()
logdown::config_netlify()
blogdown::config_netlify()
blogdown::hugo_build()
knitr::opts_chunk$set(collapse = TRUE,warning=FALSE,message=F)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt),count(weater_code))
library(tidyverse)
library(tidyverse)
data <- read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/data/london_bike_sharing_data/london_merged.csv")
data <- read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/data/london_bike_sharing_data/london_merged.csv")
data <- read.csv("C:/Users/Oskar/Documents/R/london_bike_sharing_data/london_merged.csv")
head(data)
data <- data %>%
mutate(date = as.Date(data$timestamp),
weather_code = as.factor(data$weather_code))
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt),count(weater_code))
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt),count(weather_code))
cnt_by_weather
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt),count(weather_code))
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(mean(cnt))
cnt_by_weather
lm(cnt~weather_code -1,data=cnt_by_weather)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(cnt=mean(cnt))
cnt_by_weather
lm(cnt~weather_code -1,data=cnt_by_weather)
cnt_by_weather <- data %>%
group_by(weather_code) %>%
summarise(sum_cnt=sum(cnt),mean_cnt=mean(cnt))
cnt_by_weather
head(data)
data
by_date = data %>%
group_by(date,weather_code) %>%
summarize(cnt=sum(cnt))
head(by_date)
by_date %>%
ggplot( aes(x=dater, y=n, group=weather_code, color=weather_code)) +
geom_line()
by_date %>%
ggplot( aes(x=date, y=n, group=weather_code, color=weather_code)) +
geom_line()
by_date %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
head(by_date)
by_date %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
by_date = data %>%
group_by(date,weather_code) %>%
summarize(cnt=mean(cnt))
by_date %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
by_date = data %>%
group_by(date,weather_code) %>%
summarize(cnt=sum(cnt))
head(by_date)
by_date %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
data %>%
group_by(date,weather_code) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
suppressMessages(library(dplyr)) # Package for data transformations and tables
library("RColorBrewer") # for pretty colors
library(caret) # Fitting ML models with CV and more
library(caret) # Fitting ML models with CV and more
library(MLeval) # for plotting ROC curves and more
install.packages("MLeval")
library(MLeval) # for plotting ROC curves and more
colors = brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10)];
options(repr.plot.width = 12, repr.plot.height = 12, repr.plot.res = 100) # plot size
set.seed(12332) # set the seed for reproducability
eBayData = read.csv('https://github.com/mattiasvillani/MLcourse/raw/main/Data/eBayData.csv', sep = ',')
eBayData = eBayData[-1] # Remove a variable that we will not use.
eBayData['Sold'] = as.factor((eBayData['Sold']==1)) # Changing from 1->TRUE, 0->FALSE
levels(eBayData$Sold) <- c("notsold","sold")
set.seed(123)
inTrain <- createDataPartition(
y = eBayData$Sold,
p = .75, # The percentage of data in the training set
list = FALSE
)
training = eBayData[ inTrain,]
testing = eBayData[-inTrain,]
glmFit = glm(Sold ~ ., family = binomial, data = training)
yProbs = predict(glmFit, newdata = testing, type = "response")
2
threshold = 0.5 # Predict Sold if yProbs>threshold
yPreds = as.factor(yProbs>threshold)
levels(yPreds) <- c("notsold","sold")
confusionMatrix(yPreds, testing$Sold, positive = "sold")
suppressMessages(library(dplyr)) # Data transformations and tables
library("RColorBrewer") # for pretty colors
colors = brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10)]
set.seed(123342)         # set the seed for reproducability
bikes = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/BikeShareData/hour.csv")
bikes$dteday = as.Date(bikes$dteday) # convert date column to proper date format
bikes$logrides = log(bikes$cnt)      # we model the log(number of rides) as response.
bikes$hour = bikes$hr/23             # hour of the day. midnight is 0, 11 PM is 1.
bikesTrain = bikes[bikes$dteday >= as.Date("2011-02-01") &
bikes$dteday <= as.Date("2011-03-31"),] # Data from feb and march 2011
# Function that computes the basis function for a vector of x-values.
# order=2 (quadratic) in the example above.
PolyMatrix <- function(x, order){
X = cbind(1,x)
if (order==1){return(X)}
for (k in 2:order){
X = cbind(X, x^k)
}
return(X)
}
# Training a polynomial regression model of the 8th order.
order = 8
XTrain = PolyMatrix(bikesTrain$hour, order)
yTrain = bikesTrain$logrides
betaHat = solve(crossprod(XTrain),crossprod(XTrain,yTrain)) # Least squares estimator
yFit = XTrain%*%betaHat
# Function that trains a polynomial model,
# computes predictions over fine grid of values xGrid
# and plots the fit and training data.
PolyPlotFit <- function(x, y, order, xGrid){
X = PolyMatrix(x, order)
betaHat = solve(crossprod(X),crossprod(X,y))
Xgrid = PolyMatrix(xGrid, order)
yFit = Xgrid%*%betaHat
plot(x, y, pch = 16, cex = 0.5)
lines(xGrid, yFit, col = colors[2], lwd = 2)
legend(x = "topleft", inset=.05, legend = c("Data", "Fit"),
lty = c(NA, 1), lwd = c(2, 2), pch = c(16, NA),
col = c("black", colors[2]))
}
xGrid = seq(0, 1, length = 100)
PolyPlotFit(bikesTrain$hour, bikesTrain$logrides, order = 8, xGrid)
head(bikesTrain)
bikes$dteday
bikes$hour
PolyPlotFit(bikesTrain$hour, bikesTrain$logrides, order = 8, xGrid)
data %>%
group_by(date,weather_code) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
data %>%
group_by(date) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt)) +
geom_line()
data %>%
group_by(date) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt)) +
geom_line() + theme_bw()
data %>%
group_by(date,weather_code) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt, group=weather_code, color=weather_code)) +
geom_line()
data %>%
group_by(date) %>%
summarize(cnt=sum(cnt)) %>%
ggplot( aes(x=date, y=cnt)) +
geom_line() + theme_bw() + ggtitle("")
bikes = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/BikeShareData/hour.csv")
bikes$dteday = as.Date(bikes$dteday) # convert date column to proper date format
bikes$logrides = log(bikes$cnt)      # we model the log(number of rides) as response.
#new code
head(bikes)
bikes = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/BikeShareData/hour.csv")
#new code
head(bikes)
bikes$dteday = as.Date(bikes$dteday) # convert date column to proper date format
#new code
head(bikes)
#new code
(bikes)
bikes = read.csv("C:/Users/Oskar/Documents/R/london_bike_sharing_data/london_merged.csv")
bikes$dteday = as.Date(bikes$dteday) # convert date column to proper date format
bikes
bikes$dteday = as.Date(bikes$timestamp) # convert date column to proper date format
bikes$logrides = log(bikes$cnt)      # we model the log(number of rides) as response.
bikes$hour = bikes$hr/23             # hour of the day. midnight is 0, 11 PM is 1.
bikes = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/BikeShareData/hour.csv")
bikes = read.csv("C:/Users/Oskar/Documents/R/london_bike_sharing_data/london_merged.csv")
bikes$dteday = as.Date(bikes$timestamp) # convert date column to proper date format
bikes$logrides = log(bikes$cnt)      # we model the log(number of rides) as response.
#new code
(bikes)
head(bikes)
bikes$hour = bikes$hr/23             # hour of the day. midnight is 0, 11 PM is 1.
head(bikes)
?as.Date
blogdown:::serve_site()
blogdown:::new_post_addin()
knitr::opts_chunk$set(collapse = TRUE,warning=FALSE,message=F)
getwd()
data = read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/data/HotelBooking/hotel_bookings.csv")
head(data)
knitr::kable(head(data),"Head of Data")
library(knitr)
knitr::kable(head(data),"Head of Data")
install.packages("knitr")
install.packages("knitr")
library(knitr)
knitr::kable(head(data),"Head of Data")
library(knitr)
knitr::kable(head(data),"Head of Data")
kable(head(data),"Head of Data")
kable(data)
kable(head(data),"Head of Data")
knitr::kable(head(mtcars[, 1:4]), "simple")
head(data)
kable(head(data),"Head of Data")
knitr::kable(head(mtcars[, 1:4]), "simple")
kable(head(data),"Head of Data")
kable(head(data),caption =  "Head of Data")
knitr::kable(head(mtcars[, 1:4]), "simple")
kable(head(data),"simplte", caption =  "Head of Data")
kable(head(data),"simple", caption =  "Head of Data")
str(data)
kable(str(data))
kable(head(data),"simple", caption =  "Head of Data")
blogdown:::serve_site()
data
sum(data$hotel=="Resort Hotel")
sum(data$hotel!="Resort Hotel")
table(data$hotel)
data$reservation_status_date
data$reservation_status_date = as.Date(data$reservation_status_date)
str(data)
str(data)
boxplot(data$adr)
ggplot(data, aes(x=hotel, y=adr)) +
geom_boxplot()
library(tidyverse)
ggplot(data, aes(x=hotel, y=adr)) +
geom_boxplot()
ggplot(data, aes(x=hotel, y=adr)) +
geom_boxplot() + theme_bw()
data$adr>5000
sum(data$adr>5000)
sum(data$adr>2000)
data[,"adr"]<2000
data = data[,"adr"]<2000
sum(data$adr>2000)
data$adr
data
data = read.csv("C:/Users/Oskar/Documents/R/OskarMikko/OskarMikko/static/hotel_bookings.csv")
index = data[,"adr"]<2000
data = data[index,]
sum(data$adr>2000)
